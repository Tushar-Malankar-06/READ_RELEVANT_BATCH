{
  "newsletterName": "the@Batch",
  "date": "2025-06-19",
  "content": [
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "Apple Sharpens Its GenAI Profile",
      "content_String": "Apple revamped two vision-language models in a bid to catch up with fast-moving competitors. What’s new: Apple updated the Apple Foundation Models (AFM) family, including smaller on-device and larger server-hosted versions, to improve their capabilities, speed, and efficiency. It also released the Foundation Models framework, an API that enables developers to call the on-device model on Apple devices that have Apple Intelligence enabled. Input/output: Text, images in (up to 65,000 tokens), text out Architecture: AFM-on-device: 3 billion-parameter transformer, 300-million parameter vision transformer. AFM-server: custom mixture-of-experts transformer (parameter count undisclosed), 1 billion-parameter vision transformer. Performance: Strong in non-U.S. English, image understanding Availability: AFM-on-device for developers to use via Foundations Models framework, AFM-server not available for public use Features: Tool use, 15 languages, vision Undisclosed: Output token limit, AFM-server parameter count, details of training datasets, vision adapter architecture, evaluation protocol How it works: Introduced last year, AFM models use a vision encoder to produce an image embedding, which a vision adapter modifies for the LLM. The LLM takes the modified image embedding and text prompt and generates a response. The team trained the systems to predict the next token, align embeddings produced by the vision encoder and LLM, and align responses with human feedback. They trained the models on text and image-text data from publicly available datasets, data scraped from the web, and data licensed from publishers. Quantization: The team used quantization aware training (simulating quantization during training to improve performance of the quantized model at inference) to compress AFM-on-device to 2 bits per weight (except for the embedding layer, which was compressed to 4 bits per weight). They used Adaptive Scalable Texture Compression, a method initially designed for graphics pipelines, to compress the AFM-server model to an average of 3.56 bits per weight (except for the embedding layer, which is compressed to 4 bits per weight). LoRA adapters: They trained LoRA adapters to recover performance loss due to compression, which adapted the model to specific tasks including summarization, proofreading, replying to email, and answering questions. MoE architecture: While AFM-on-device uses a transformer architecture, AFM-server uses a custom mixture-of-experts (MoE) architecture. A typical MoE can be viewed as splitting a portion of its fully connected layers into a number of parallel fully connected layers, of which it uses only a portion at inference. In comparison, the AFM-server’s MoE first splits the model into groups of layers, then it splits each group into parallel blocks. Each block is a separate multi-layer transformer outfitted with MoE layers (processed on a small number of hardware devices). While a typical MoE combines results across all devices at every mixture-of-experts layer, Apple’s architecture combines them only at the end of each block, which saves communication overhead during processing. Performance: In human evaluations, the AFM models achieved mixed performance compared to selected models of similar or greater size. The tests included language tasks in U.S. English, non-U.S. English (including Canada and UK), and a basket of European and Asian languages. AFM-on-device: The on-device model performed better than the competitors at language tasks in non-U.S. English and image understanding. For instance, answering questions about images, AFM-on-device bested Qwen2.5-VL-3B more than 50 percent of the time and was judged worse 27 percent of the time. AFM-server: The server model’s performance was not decisively better than that of the competitors. For instance, AFM-server outperformed Qwen3-23B 25.8 percent of the time but was judged worse 23.7 percent of the time. It underperformed GPT-4o in all tests reported. Behind the news: Apple dominated social media last week with a controversial paper that purported to show that 5 state-of-the-art reasoning models couldn’t solve puzzles beyond a certain level of complexity. The researchers prompted the models with four puzzles that allowed them to control complexity, including swapping the positions of red and blue checkers on a one-dimensional checkers board, Tower of Hanoi, River Crossing, and Blocks World. For all the puzzles and models, they found, the models’ performance fell to zero when the puzzles reached a certain degree of complexity (for example, a certain number of checkers to swap). A rebuttal paper quickly appeared, penned by Open Philanthropy senior program associate Alex Lawsen with help from Claude 4 Opus. Lawsen contended that Apple’s conclusions were unfounded because its tests included unsolvable puzzles, didn’t account for token output limits, and posed unrealistic criteria for judging outputs. However, he later posted a blog, “When Your Joke Paper Goes Viral,” in which he explained that he intended his paper as “obvious satire” of authors who use LLMs to write scientific papers, and that he hadn’t checked Claude 4 Opus’ output. He updated his paper to correct errors in the original version but maintained his fundamental critique. Why it matters: Apple has been viewed as falling behind in AI. A promised upgrade of Siri, Apple’s AI assistant, is delayed indefinitely, and the lack of advanced AI features in new iPhones has led to a class-action lawsuit. Meanwhile, Google and its Android smartphone platform are racing ahead. The new models, especially the Foundation Models framework, look like a bid for a reset. We’re thinking: Apple may be behind in AI, but its control over iOS is a huge advantage. If the operating system ships with a certain model and loads it into the limited memory by default, developers have a far greater incentive to use that model than an alternative. Limited memory on phones and the large size of good models make it impractical for many app developers to bundle models with their software, so if a model is favored by Apple (or Android), it’s likely to gain significant adoption for on-device uses.",
      "sourceUrl": [
        {
          "text": "nbsp;\nApple revamped two vision-language models in a bid to catch up with fast-moving competitors.\nnbsp;\nWhat’s new: Apple updated",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6P3qgyTW8wLKSR6lZ3kTW10Kyb57Y0gy0W1-YB1l1G6xxBW2JvYw5727Sx7W5QWSLL96JLj3W8f1NQS5lkX2sW7tYdf36n7XXdV1H1hR5C4Xc5N73n4W97VLGSW3zxbxj4TMc55W9f89Nr8ZppkgW60VH994zlMqyW8rf5-X8bMsknV41gM85PJvVxW45nSkQ8lm_syVFB7b55kNYbSN5sTChmlK3bqW5CFCcf1xMN4wW1jmJqp8dbLKKW5WFsgD2456BbW2jSNmw1B58xLW2XD8ls7Rlq9CW3h8-k62TnqQwW6Tq7JJ45bhvnF3VHZCtLKYgVtHZL01HbYvMVw7m1W6f84l0W3q_2Lx77-JKPW5PGBF65TTq_gf1G6mjn04"
        },
        {
          "text": "the Apple Foundation Models (AFM) family, including smaller on-device and larger server-hosted versions, to improve their capabilities, speed, and efficiency. It also released the Foundation Models framework",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3nSVtx9vt3cSk5TW5q9X3527dnFfVQgwKt6KY7YyW68Q1jB7rQ6fcW4z4LZp6S6n1VW6yjdH_8LDCnHW8FLQWQ2smH2wW1Pj4FD86N58XW1zMYLf5TK-K_W252l158vKpl6W8M1p2Q5sLz0kN8k3z_QMKzZLW8zFRYB2XNvzYW4g-rJc1WP1kRW6DcsBf81yKV2W52VHKD58w9jDVPHqrR7MDqQQN81dCh_RKLHkW2Z_wrC2qq1YTW8gCPT84ggRqgN8xNvXTXytjdVylgzm50BJD5W6bgcQN777Ds5W2QRwkV1tQ2l2f7CZ1mY04"
        },
        {
          "text": ", an API that enables developers to call the on-device model on Apple devices that have Apple Intelligence enabled.\n\nInput/output: Text, images in (up to 65,000 tokens), text out\nArchitecture: AFM-on-device: 3 billion-parameter transformer, 300-million parameter vision transformer. AFM-server: custom mixture-of-experts transformer (parameter count undisclosed), 1 billion-parameter vision transformer.\nPerformance: Strong in non-U.S. English, image understanding\nAvailability: AFM-on-device for developers to use via Foundations Models framework, AFM-server not available for public use\nFeatures: Tool use, 15 languages, vision\nUndisclosed: Output token limit, AFM-server parameter count, details of training datasets, vision adapter architecture, evaluation protocol\n\nHow it works: Introduced",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR743qgyTW95jsWP6lZ3m1W69HKZT7RJl0lV8q9xQ2vz5TbN5Md2D9cr32fW7Ffc2q82m08KVl5Wpg4TvGWdW30tF_83-_cWmW1WCgY_4QnLM4W6DSTYK1XhWfbW71qJ0-2bN1qXW2ZVfrh63tmGwW6t0Jzm26ssPLW2WG67q3BHVx-W1QkXPq551lxzW7wjv3R2h2t-5W6RS4DY4wf74PVj1G0W3wkl_4VKtLFM5jX-K1W1x69_859Jz1hN4TV90QJdnzXF8d9_795qXrW1BMshT2VdwkNW1jMJSk1vrDL5W4gLgbY5KCBlGW27sCzf7v1cvQW25NS5g8p8RKPW7NP_BD59pSXPW3P-BwM1FzfzPW3-F4vr69zxWLW1gCkHT3czK-cN1Wp0t566k-ff5jZP4s04"
        },
        {
          "text": "last year, AFM models use a vision encoder to produce an image embedding, which a vision adapter modifies for the LLM. The LLM takes the modified image embedding and text prompt and generates a response. The team trained the systems to predict the next token, align embeddings produced by the vision encoder and LLM, and align responses with human feedback. They trained the models on text and image-text data from publicly available datasets, data scraped from the web, and data licensed from publishers.\n\nQuantization: The team used quantization aware training",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3kSW7QcC442GhjRqN4GpHgPjb_whN1cnkSx91Fw1N3tpMQqyq1Q2W6Y9rw83v7PFpW945s2g2YVSHJW7dKLVt7Tb06JW6nkTwk5PcdR9W8TpCG86N3H4NW5gcC4H3mmMKcN7prWdrWMwRwN3Z9dZRLjl80W1s2Vv11ywQmYW7JqDTy8n7BPWW2hb7Nk9jfKTJW80lks43T3t3vW4z-CrB8Hm-SVW1VyNX-7851DtVdMgJy2l4ggyW6cpV2D8kP3BKW8rDfzc7C7l0BW6t6P-G909d3mf97R8CW04"
        },
        {
          "text": "(simulating quantization during training to improve performance of the quantized model at inference) to compress AFM-on-device to 2 bits per weight (except for the embedding layer, which was compressed to 4 bits per weight). They used Adaptive Scalable Texture Compression",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6v3qgyTW7Y8-PT6lZ3p1W7vBdxQ3_T5jcW8s4PKX3rTn1BN1k8L73qKK8gW30KLXV7YDFmnW2_GW1b7q4h81W6Gs8kN8bzr5wW5DsrCw4xdYPKW4S9SK08sHq8bW68l8XS6xmR8fW1TRW095lNjg1W53MxGr3SlpRHN5H6mQs34WR7W6Xvw727vDbXVW5Hhlt78GvMKYW4-Pm5J964GQ_W1qbDQH65Fs3rN6TdH8P7TLqjW7_G0MF1C9vY3VDldnH98x8CWW3X_lzy9kPTxkW324NSr8Tyn_vW7s3qM14HsXDLVch9gZ1qxyp1W8RzM0V74nn0ZW1qzgYQ2ZTk0cW2HVdKQ5p3bBvf52W1mv04"
        },
        {
          "text": ", a method initially designed for graphics pipelines, to compress the AFM-server model to an average of 3.56 bits per weight (except for the embedding layer, which is compressed to 4 bits per weight).\nLoRA adapters: They trained LoRA adapters to recover performance loss due to compression, which adapted the model to specific tasks including summarization, proofreading, replying tonbsp; email, and answering questions.\nMoE architecture: While AFM-on-device uses a transformer architecture, AFM-server uses a custom mixture-of-experts (MoE) architecture. A typical MoE can be viewed as splitting a portion of its fully connected layers into a number of parallel fully connected layers, of which it uses only a portion at inference. In comparison, the AFM-server’s MoE first splits the model into groups of layers, then it splits each group into parallelnbsp;blocks. Each block is a separate multi-layer transformer outfitted withnbsp;MoE layers (processed on a small number of hardware devices). While a typical MoE combines results across all devices at every mixture-of-experts layer, Apple’s architecture combines them only at the end of each block, which saves communication overhead during processing.\n\nPerformance: In human evaluations, the AFM models achieved mixed performance compared to selected models of similar or greater size. The tests included language tasks in U.S. English, non-U.S. English (including Canada and UK), and a basket of European and Asian languages.nbsp;\n\nAFM-on-device: The on-device model performed better than the competitors at language tasks in non-U.S. English and image understanding. For instance, answering questions about images, AFM-on-device bested Qwen2.5-VL-3B more than 50 percent of the time and was judged worse 27 percent of the time.\nAFM-server: The server model’s performance was not decisively better than that of the competitors. For instance, AFM-server outperformed Qwen3-23B 25.8 percent of the time but was judged worse 23.7 percent of the time. It underperformed GPT-4onbsp;in all tests reported.\n\nBehind the news: Apple dominated social media last week with a controversial paper",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6v3qgyTW7Y8-PT6lZ3l3N5_sBG2l3tvrW31vx5y4w68Y0W8BQN0z8Xr1dKW7mByHn3Ph9_-N5Kkhb8h07YLN2DCCVZ3DybvN8mvLby3lzYLW2qLdQK5TGpsYW6p-wH-6L1gL9F9hjP9XrZDhN6cLGgvmZSDrW37BRbb8VpGdNW3r273v36HXdxW6hqGG82wlpwdN3B17X0rQJKRW3Q-tQW2KhV3GW8SxkZr8fy2tlW4spVsH1cYTT-W8bC9Jh9bXjkLW63_h1g37CRYjN1fgsMVwrLlTW2DsHGt47f_1ZW3KzJWm4g-r__W7Cvcvg5_WgGWW3TbtKd45f5PVN1mTCXLfy42vf4fnFz-04"
        },
        {
          "text": "that purported to show that 5 state-of-the-art reasoning models couldn’t solve puzzles beyond a certain level of complexity. \n\nThe researchers prompted the models with four puzzles that allowed them to control complexity, including swapping the positions of red and blue checkers on a one-dimensional checkers board, Tower of Hanoi",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3kPMTlRFglq3xLW5XG_6t5wz9FWW1qBHjc7_-BG-W8MvG9Q4KY6f0W7T21tg6RjnhkN95jBNK3Nmq7W3Kn-7f2ZzTlbVFMhYt7j5_1SM7j0HnWvM9SW3wRJg91xWhMDN3Dl8XNkDPJwW3kB0Ls76sS-JW6_27hw4fcPK7W5YJ9kN98PJhwW7llWZS92HMbsW2sdtHv5-0XhPW5-gmg12Mw9yVN2Vkfl6Rpr2ZW2YGl6N2kp0zHW6VyfGg5K-WVcW1T8SDz92yBHwW2_mlWb1mxtsNW2txj425FGcrbW4Ls9977gDJHVf2fjx0004"
        },
        {
          "text": ", River Crossing",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3pMW6J-11826S3TSW6M5TYQ5MB5xWW74c8Y69bp3YnW28sBny5f8djdVtqk-C3wj45YW2Mz88Z2zq3BKN2CKjRQMNlQ3W58tNJ770dprSW4Wvxwj4TqZHSW1FLzZh5-C4Y-W2Mb3Kq2L6NzSW1l2mZm43QV0pW4ngTsk63TqshVhSb8B4mlCFGW274WWP2wT1R3W87Y9Fc1X9CJWVfHkm18-5nGhW6SVq257RJpf8W5l3BBW8cs76cW7d2Rsl7V1_N9N7QP35BkMSHhVSVmmc11cp8FW5zY8X7465P28W5c9s317zJmdZf5HGVWx04"
        },
        {
          "text": ", and Blocks World",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3l4W62LbXS158qFzW301DkL15WG2qW5vKb4x3Rl0yJW2RJ8tG6CqL_FW2k8XPb4DR4QjW1NGQDZ2RPDdTW2Hgqhg6d0bxjVJ9J5D8Pk6L8V4C0z17LfF2cW4lLh471kjRlrW8Qcvgh44NV3GW3Fn6bK7R62YXW6fJtX65JBKgMN5hXmqtchhlmW4BZ9hr2c6SkxW35KmS18l__3ZW1XLSl01FPT4XVcPW8Q6Mhmy-W2hzFLP7QfgzNW3RSYdf1mJlTqW5Jqryq6RyTTFW18Vgnc1MWgVsf1KTNV204"
        },
        {
          "text": ". For all the puzzles and models, they found, the models’ performance fell to zero when the puzzles reached a certain degree of complexity (for example, a certain number of checkers to swap). \nA rebuttal paper quickly appeared, penned by Open Philanthropy senior program associate Alex Lawsen with help from Claude 4 Opus. Lawsen contended that Apple’s conclusions were unfounded because its tests included unsolvable puzzles, didn’t account for token output limits, and posed unrealistic criteria for judging outputs. However, he later posted a blog, “When Your Joke Paper Goes Viral",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6v3qgyTW7Y8-PT6lZ3ljW839C5X7k2q7ZW4Sy1wh8VYsHwW4bxGTf8ZC4gNW3r0QZq2cSRl6VPfTJY5bNnR8N7CVsH-PMJmPW1P4gKm1wWnl2W8ZXjLL3N7MBcW58y5t15N54YRW7MXHD76H247SW7xbN7_4YjRGmW53z3F67s0CqTVLrgzc8gtrLKW8JbKvQ3D5fHVW1K9vp58hpS-5W1JljPm5mkywDW8qT69X4X2RtNW1x7Lj34PVgthN360sx1hf8ZqN4H_qqCKQsnHM-P_zmhVnk7W67HB_55STM3-W873cVk14m61YW3yzb9f8X_SrMW7g8_KK6rGH9nW8hRpL75JxkXRf5vzN7Y04"
        },
        {
          "text": ",” in which he explained that he intended his paper as “obvious satire” of authors who use LLMs to write scientific papers, and that he hadn’t checked Claude 4 Opus’ output. He updated his paper",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3pCW8zbNPq70XF2_N6wPglBDymn0W6CnBmG4HSzWjW7VNwDM9bKX4JW5FJmlR7wGgLZW7pw4cc3Q7Q7jW7MQG282StZ2MW2W9Yct562NvxW9gvZfV2VT9RrN7-4KbSF3dZxTNTp46DGPqVW43Bpcr7Pf60nVkBcWD6qZC09W2TSZNQ1vmHqcW7vmFZ72cFyN6W8C_Vh77kCkGCW8rdZ_F5s7_C9W2LGbKJ77mrDzW1b_sBD1bjhR9W5Bjwgj9fW108N1Yg5s9fl3XtW5FNTp55szqM2f8LCKNl04"
        },
        {
          "text": "to correct errors in the original version but maintained his fundamental critique.\n\nWhy it matters: Apple has been viewed as falling behind in AI. A promised upgrade",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR743qgyTW95jsWP6lZ3mPW4FT33w4h2rbPW1pq9-v7glDfQW12Mv1T1Q4wMzVjvwmz4FX6RlW2hPR4q82xH_PW1lNqmX5XcSbMW3MnHxk2zNQhvW7SfypT314My8N2mJCXzgZk_cW2bdnLt4G3svSW8sp6Jg715NFKVXsYw_92LRLtW28JGnC5DzRK_N4XqmZN5fFw1W6CY3XV38LpZ6W7jlZhR41C8cjMZSQHJK92tjVG4b2K4tMlG1W1ZNtpx6VG7pqW4yqbFD6w9gpXW6t47_C7tmLJrW3sQt_Z6g4p6PW17QKW36d70vzW70VPTB5Z0W0qW9lKlNN8XbfsRV-k8x42ZN3ZdW76Bwfh4ZJ-2vW7YWdSn5SMPGRW8wdS9f8gw9fcW48GJsH1VS_csdMVRPd04"
        },
        {
          "text": "of Siri, Apple’s AI assistant, is delayed",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR505nR32W50kH_H6lZ3p5N6W8rBS2kXh5W1-3GXV221fbGN8sl2QJYgdWfW6_140x8xBfZ7N2tD97NG5gpRW1lC7g-7FWMSvN5q04hl5pWzsN60ZSyNPB_RkN6TczrDhL5l_W4D-qVT69gVbrW69x4Vn1hWXxRW8gGc9M2WrMt5W31X28h4K6KctW8cqY6x7bX075W44t-L614Z5GNW5Vz9-L7MN-tyW7GFd-f90dHypW4wmKs-6bGYmMN3jjTGYG1vyMW3152nR4Fy2xrW1LnxLT4rGRwGW3yT59r8qPJ38W4Nz-lg4qrTLwW4ww2pg3zBVJdVBqm8B3YX-3tW5CNN7d7zzKHZW7y7zDJ7-sJ3QW3625tY58PWXSW5397qN4L-GvxW8nk93_5Jfy_bW8cT2S64q4Z6lW1nzpth2g9fT9f3zpPMW04"
        },
        {
          "text": "indefinitely, and the lack of advanced AI features in new iPhones has led to a class-action lawsuit",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6v3qgyTW7Y8-PT6lZ3kqW2xhTKk43P0BkW6M9KV58d14nSW7kP6Hj720PBjW6d5zQy1RVlQdW17dLQ14RZMs9W6CyTyH8jlCP5W2b94BZ1nbM1SW4mzvLx626KSWW7vyLWM3H1rBQW9lCNYx6TQ1whN5l4Cq76z95tW6YH5c-3t-yDyW1T4dsb4nVpbdW4KHtP51yJPGdW66xy577S-m5SW847pBY4PLVk6W53mJXC2mD8cdVR6yRZ5rGGRmW7nSvng54KcGZN31HkCYbHjvCW7MKmvJ3JLnpKW3M6Jh05BPB37W2n2cYt7mfLV2W7Rv_kH3vq_70W43MjBS8G2XdkW4vRXfs7mD-X9f4DBY-804"
        },
        {
          "text": ". Meanwhile, Google and its Android smartphone platform are racing",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR505nR32W50kH_H6lZ3nnVtNGp-6NFw0VW8RPQMk4Pyb4nW5w_jZW5pq_fGW6CwDJQ3S4Pz7W3rF4s-8xDVb_W7vsZyl14Wg9-W81H6sf8nrsf4W2dC1b62zfy5tW4WKM_62v-K2GW6xGBR65r_HBHVDX-4B923M6SW8hFwDP4LQR14N3G__fYVWtxjW8LT_w33FnwZ4VR7W2D8SlndQW2WywJ64dwS4BW6fn-pP87kXRSW6wLwTd1zct4pW8JjtCF15X35lVP6hXb2nTyMgN4_P_CT4by8WW2nClyd7Nt3Z_W4SFnMj7h8SzsW5SZZnR1x2bKpW6Zqg7279Vb-0Vwmj8X8C17CcVGmP_f75xm3kN7JBQJxXBKrgW557qZP178ByqW6GMwnB6kW4svW5Nypgt7NvjSQW89MThy1crzn7f3ngPx004"
        },
        {
          "text": "ahead",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W5nR32W6N1X8z6lZ3pxW7KTf-r5yYf-FW3dQqFr8sPlyLN1rg9jT7cY8mW6Qmn8580TX_8W6JzZjm1SWCWmW4dlnwQ5ffy-jVwvz8-53Q3JQW24MX9F10mRbNW5_bBHv8qKNmvW1b9Gj8939qW-W1m_nnj6-l3mSW6Lb3336C7r0_W7ck_jy3dK4FYVBBHTr2h_W7hW1PPR0y7VCCz-W79rJDj75wxq9VKMzjB97l8D3W2W-1_f3dM433W7MVkxL5yK_k5W81tjnj5n6pqjW7yRmcX2X7QRKW4_2Krw4RsRdPW85yTJd32SLTlW7QCMw28GrnMTW2N5SDP5vb7ZVW83jw6R1wv5PGW2gg6QL9hZMdpW2_3QgL4D6WHDW1ZFywQ6N344VW87p7mv7GGwqyW3GGVgV2vS95fW3KcWw978Glg7W7BPYVf17S64nN66gPkFslRW_W8f4Zc52jK5y_N6zwcLQbphZYW1xNQ7Q5pFyk3W1fqH_F4DNXTgf5jKdln04"
        }
      ]
    },
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "Hollywood Joins AI Copyright Fight",
      "content_String": "Hollywood studios joined the record companies, publishers, and artists in the fight against companies that have trained AI models on their copyrighted works. What’s new: Disney and Universal sued Midjourney, accusing the image-generation startup of training its models on “countless” unauthorized copies of their copyrighted works and distributing images that depict characters the plaintiffs created. How it works: Disney and Universal asked the court to order Midjourney to cease its alleged unauthorized distribution of their intellectual property. Further, they want Midjourney, which took in revenue of $300 million in 2024, to pay unspecified damages based on the claim that copyright law entitles them to $150,000 per infringed image. The studios accuse Midjourney of both direct infringement (that is, directly violating their copyrights by copying, displaying, or distributing their work without permission) and secondary infringement (enabling or encouraging direct infringement by others). The lawsuit alleges that Midjourney reproduces copyrighted and derivative works, including the images of movie and television characters from Star Wars, Toy Story, Cars, Ironman, and The Simpsons. Midjourney generates such images even if users don’t ask for them explicitly. For instance, an image allegedly generated in response to the prompt, “Superhero fight scene,” includes Disney’s Spider-Man character. Midjourney is aware of the infringement, the studios claim, pointing out that Midjourney’s website includes infringing images in sections curated by the company. Midjourney could use software that would prevent its system from generating and distributing copyrighted material, the lawsuit says, citing other software products that identify copyrighted works automatically. The filing alleges that Disney and Universal sent cease-and-desist letters to Midjourney, but the AI company didn’t stop producing and distributing images that infringe their copyrights. Behind the news: Copyright law is ambiguous on whether training AI systems on copyrighted works requires permission from the copyright holders, and several cases are winding their way through U.S. courts to answer this question. Starting in 2023, artists, authors, and publishers initiated legal actions against Alphabet, Meta, and OpenAI. Last year, some of the largest companies in the recording industry sued the AI music startups Suno and Udio. In February, a Delaware federal court issued the first major decision in this area, when a U.S. Circuit judge ruled that an AI-powered legal research service could not claim that training its models on writings produced by Thomson Reuters was a fair use because the resulting products competed with Thomson Reuters’ own products. Why it matters: AI systems require enormous amounts of data. Historically, developers have felt free to use whatever copyrighted works they could find, typically online. As AI systems show greater potential to erode the market for human-made creative works — and to reproduce such works and create new works derived from them — owners of copyrighted material are looking for compensation as well as protection against this new form of competition. A single lawsuit won’t settle the issue, but this case, brought by two of the most powerful entertainment companies in the world, could set a precedent that strongly influences future lawsuits, the behavior of AI companies, and future legislation to update copyright for the AI era. We’re thinking: Film studios and music labels once considered YouTube a copyright violator. Viacom, the entertainment company behind MTV and The Jersey Shore, once sued YouTube for copyright infringement. YouTube prevailed in two proceedings before the parties settled out of court, and YouTube subsequently improved its ability to detect and remove copyrighted works. Today, movie and recording companies rely on the enormously popular web video service to promote their wares. Given that history, Hollywood might consider partnering with AI companies instead of suing them. The pie would be bigger if Hollywood and AI companies worked together, although how to divide it would need to be worked out.",
      "sourceUrl": [
        {
          "text": "nbsp;\nHollywood studios joined the record companies, publishers, and artists in the fight against companies that have trainednbsp;AI models on their copyrighted works.\nnbsp;\nWhat’s new: Disney and Universal sued",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6P3qgyTW8wLKSR6lZ3n7W6Bl1JN8Vx82gVCfFXJ1dFj2KW6TBWyF7mvZLlW8TfkCs88qK1HW7f46g87X-J9sW4gYlzC14vTFJW2xt6qq2dxypRW1Js8xk5LMRKkW7Bn1ZY3tt80MW5WHSwB8C0qq1W4gPGyT6G_CJ2W4h2XfC8ndFgRN57yLlqGPSF4W9hlR8d7DmmhWW6k7PFh1-1dxDW8kGlZD3cG9N3W1mZ0bc2mBbznW2mtRHL2JTF_2W5pvPgd3bS88ZW3cTcxc8HGfclVCslMv73Rjd-Vfyvvp1LLJLkW4vXMT48XK7RlW5dGlhY4jBNVBVLwYG_7Hb8X4V93gKB1QRJK1W1mLztR4cM1MxW6NvXHW3_zB7Nf6vQWnC04"
        },
        {
          "text": "nbsp;Midjourney, accusing the image-generation startup of training its models on “countless” unauthorized copies of their copyrighted works and distributing images that depict characters the plaintiffs created.\nnbsp;\nHow it works: Disney and Universal asked",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6P3qgyTW8wLKSR6lZ3pTW44LykF2ZbQnVW8rq_d_5X8c1JW4DvXs24J2mLfW2Q7XL51QCm43N1jGfc-XNpw_W4h6mf11PN-GlW4DYlC_6jm416W9bmnPX38lv5nW9cjyYS8zkttpW96wSHP7hDW-BW6C6tSS25hZNkN14MYmmhBLDJN6GCzPg7yzPDN32L6xHgX2YpN8nqTwGFmnLpVMRvCS3LN8_pW2Hycy-2pnQDgW6V3V3x5WLSGBV6qSp-7l458FW2dRWM62GqknGW6Z1XtC5V9qgXW1Tghtg1BzDkbW3PZFdl68P1VFW8vP21v5nvF3bW8JvkX64z_GHTN7DWmgmFk-R1N6l4HJLC8yfpW7xgDrF359q7wf4vgzJb04"
        },
        {
          "text": "nbsp;the court to order Midjourney to cease its alleged unauthorized distribution of their intellectual property. Further, they want Midjourney, which took in revenue of $300 million in 2024, to pay unspecified damages based on the claim that copyright law entitles them to $150,000 per infringed image. The studios accuse Midjourney of both direct infringement (that is, directly violating their copyrights by copying, displaying, or distributing their work without permission) and secondary infringement (enabling or encouraging direct infringement by others).\n\nThe lawsuit alleges that Midjourney reproduces copyrighted and derivative works, including the images of movie and television characters from Star Wars, Toy Story, Cars, Ironman, and The Simpsons.\nMidjourney generates such images even if users don’t ask for them explicitly. For instance, an image allegedly generated in response to the prompt, “Superhero fight scene,” includes Disney’s Spider-Man character.\nMidjourney is aware of the infringement, the studios claim, pointing out that Midjourney’s website includes infringing images in sections curated by the company.\nMidjourney could use software that would prevent its system from generating and distributing copyrighted material, the lawsuit says, citing other software products that identify copyrighted works automatically.\nThe filing alleges that Disney and Universal sent cease-and-desist letters to Midjourney, but the AI company didn’t stop producing and distributing images that infringe their copyrights.\n\nBehind the news: Copyright law is ambiguous on whether training AI systems on copyrighted works requires permission from the copyright holders, and several cases are winding their way through U.S. courts to answer this question. Starting in 2023, artists, authors, and publishers initiated legal actions",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR505nR32W50kH_H6lZ3pGW7gppDQ7fCbLPW5FhXBX7cjvl9W5tRyKq5hgS35W8q00qS1DPWXyW5qVWV_9gj1wbW92F4-B34GrkmW8ZhYxP1_1tqdW7VhrfL4kwBRCW6XwB2w7X6D-CW7R-jkX7PKhYqW52f2vF4yPf28W5MYRS93TV-3-W35sd4B6QW2gtW8_9c5b4V7zNQN8rMpqk2zNx9W36Vz1s3MmLdpW7SG31G4sYMzKW7RXsN529lnymW3MhXRW6RmT1hW4jKgpT4-690bW6XSw5P20R-5WN40JtWK20fSzW26Pj-k7_14XyW3DcbNv6H927yN89vxQdFtdx5W67YbvM3GNLPxW34y3SR54zyY0W71-gLn5pD4LzVTRm9-6z1xXSW80clbw4v6Sg7W78P-3W2JdX0dW5bhJcn9ftb8gf7SLvkP04"
        },
        {
          "text": "against Alphabet, Meta, and OpenAI. Last year, some of the largest companies in the recording industry sued",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR505nR32W50kH_H6lZ3m6W4PBgyF3x7K7pW1Qqbh63P456XN7sfqbC-HNt6W8_W8G86DjrYqW57X19v7SQ-bGW23cSNm1QF3M7W5X5f_r8txFkHW1vs8b320GM1cW2PGc_k2K4RzqMSj4FKHKnshW5gYQXb65T60sW5C3QP97DjZ47W8D0Y8B47HYfvW6zFq_C5MVtXRW861q_m3LDPqqW1vRtk_7_vhFLV6vx3b7--BpwW2hMfz75ZqY7MW1k4nph6y1v6QW2PdfQw39b7B8VX5v6x6rGPn_W1JRdRc34SpBqW6VK88L24NcXLW55Qxc78Q2tn_W5ZBsWL1s0D9HW5rS8Bf8Xf-FNW3MLvNl88qP3RW7d1m4x97Z2f9W2KMy8d1yQqjLW8l7k742Jw-10W3QB6mr3WRXltW4YH7nK4hgD6gf6l4MSF04"
        },
        {
          "text": "the AI music startups Suno and Udio. In February, a Delaware federal court issued",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR743qgyTW95jsWP6lZ3pZW6NlHB817RC6TMT35782CzvjW8KLflY6_P9qjW2Sfn-34h0gF9W4y6DJB3rBgKjW5szdZJ3XhqRkW6SgDN04QhnNLW3SZG_q2j8DbCW6Y7fFz8g4l3tW7dnkv_2h5TJjW6xVRvR8jzzBnN24HVk-rcPnbW8Rwxxm2bGrt1W8gRTbl2n01rcW7rbS506wTmZvW2nh5Q31_YSh5W84Bdy12hBzQtW2xDgqw8l9p3QW1Y1LQL3c29lVW4ySg_85RpvYWW5c2dZj1czfrZVMRjgH3qrn-QW4kQsRC3sCkWbW4vHzQn5nvkWtW60ztzh8q9nBQW5sJ3pH1tnkDPN44ynFzBj4ZPW4VZKgk6RrPT0W4tZtc41phf_YW8SlRGP7fYfgyf8fMMNP04"
        },
        {
          "text": "the first major decision in this area, when a U.S. Circuit judge ruled that an AI-powered legal research service could not claim that training its models on writings produced by Thomson Reuters was a fair usenbsp;because the resulting products competed with Thomson Reuters’ own products.nbsp;\nnbsp;\nWhy it matters: AI systems require enormous amounts of data. Historically, developers have felt free to use whatever copyrighted works they could find, typically online. As AI systems show greater potential to erode the market for human-made creative works — and to reproduce such works and create new works derived from them — owners of copyrighted material are looking for compensation as well as protection against this new form of competition. A single lawsuit won’t settle the issue, but this case, brought by two of the most powerful entertainment companies in the world, could set a precedent that strongly influences future lawsuits, the behavior of AI companies, and future legislationnbsp;to update copyright for the AI era.\nnbsp;\nWe’re thinking: Film studios and music labels once considered YouTube a copyright violator. Viacom, the entertainment company behind MTV and The Jersey Shore, once sued",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3pXW4gz8RZ1gbTLQW44y7kN6TpF7RMrs1LxF1j4GW12bvqV6tzdfwVJn-9n3fdh2lW69ny0k2QtlM3N5bV_CyvHKjZW3kM6xw8KZHf6W8X-hNl90FYb0VbpGp27SzHBGV3CPdv6GRZqYW11G0Kf6flgQgW36fFXJ354wsLW4z2JYC6MLGDSW6pXT5b2JCtkhW8YPn3q2l3GVGW5vscFM8yly71VGQ3KZ6nt07lW90CCjt6CYp3qVBKYTm94QvnSMBndgv4hPPGW3MST792Cd7PJf2fSgf204"
        }
      ]
    },
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "Learn More About AI With Data Points!",
      "content_String": "AI is moving faster than ever. Data Points helps you make sense of it just as fast. Data Points arrives in your inbox twice a week with six brief news stories. This week, we covered how Google DeepMind is using neural networks to predict hurricanes up to 15 days in advance. Subscribe today!",
      "sourceUrl": [
        {
          "text": "nbsp;\nAI is moving faster than ever. Data Points helps you make sense of it just as fast. Data Points arrives in your inbox twice a week with six brief news stories. This week, we covered how Google DeepMind is using neural networks to predict hurricanes up to 15 days in advance. Subscribe today",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5C9cftlW69v02T6lZ3n3W45-KrQ2dxR4yW1dhj5M3G-dmyW913d9829gx3hW5fCxmg7l4Z1sV-mLK_4pNcrzW6tm7Wn4B3nG6VZtq4d8RsXgcW104THS5fPgj4W5LQ_HK3VqXL_N6Kp6dClxKY0W7fCK0k8YfDKKW5p-qhs3Tr2vPW1R9-bL5W4dzxVgg1FG8cVsSxW6yC4b52xC0-fW76HC0k2phwwfW7WTCxJ1R0CvVW5lwNBV839RnyW3m--4j19LvTKW5hmBpt5VhCRZW2d-zJL3Yqq00W1KLvhl1z-vyTW2P5WHJ5yZSv2N72fDcS727D-W3mgyGc33hblfW40c1q65MgPmgV21S707XDl65W8F-NpS6kst48W5zSgVV62Xl7sW1Vx6m87hJWQwN72L_TQ9RlssW1NfLrJ90KYbmW1jbLl67bX8ZKW5cryz66sCShXW3PdDtM5rvhBgW7j5VZ53BVGFtN2-HpbY-JbmnVZCYZN2Gz1DmW8qB24577Q0FmN3F0HgkFr-1gN2QDjBHhZ5WGW8fFWVP1kyMSVW1Fxr-z4pGN7dW98xkcY1NnXjFW3snXJl1zFhztW32J_Gp5-qm_cW190sMM2K0fbcW985yBh4NG9JGVRJPCC1Fpw-1N1gZXtN-kXwxW4n_Mcd6kbNtmW7N8Q3B5FkxymW7YgK1W4blTxWW2HQv7K886hZMW2JvKLC7l2J3fW5ny_F61vvDC2W8KZLgQ2jH1FbW2JH5hz2vK7cZN5f7zNkSPt2BW3_qc9H5tQhTpF74YShGr2DkW73QpMf5Xh-lcW7qnTYt6Dw7KzW9jJlCn5qTzzFN1VNnVZfqbxjW5KPdMH43ymR7W4-7rfb7JQRXLW3KHH201DnGGMf5DT5wv04"
        }
      ]
    },
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "More Reasoning for Harder Problems",
      "content_String": "OpenAI launched o3-pro, a more capable version of its most advanced reasoning vision-language model. What’s new: o3-pro is designed to respond to difficult challenges involving science, mathematics, and coding. But its reasoning firepower dramatically slows response times. Input/output: Text and images in (up to 200,000 tokens), text out (up to 100,000 tokens, 20.7 tokens per second, 129.2 seconds to first token) Knowledge cutoff: June 1, 2024 Features: Function calling including web search, structured output Availability/price: Available to ChatGPT Pro and Team users via OpenAI API, soon to Enterprise and Edu users, for $20/$80 per 1 million tokens of input/output Undisclosed: Details about architecture, training data, and training methods Performance: o3-pro outperformed OpenAI’s own o3 (set to medium effort) and o1-pro in tests performed by OpenAI. Solving AIME 2024’s advanced high-school math competition problems on the first try, o3-pro (93 percent) bested o3 (90 percent) and o1-pro (86 percent). Answering GPQA Diamond’s graduate-level science questions on the first try, o3-pro (85 percent) outperformed o3 (81 percent) and o1-pro (79 percent). Completing Codeforces competition-coding problems in one pass, o3-pro (2748 CodeElo) surpassed o3 (2517 CodeElo) and o1-pro (1707 CodeElo). In qualitative tests, human reviewers consistently preferred o3-pro over o3 for queries related to scientific analysis (64.9 percent), personal writing (66.7 percent), computer programming (62.7 percent), and data analysis (64.3 percent). What they’re saying: Reviews of o3-pro so far generally are positive, but the model has been criticized for the time it takes to respond. Box CEO Aaron Levie commented that o3-pro is “crazy good at math and logic.” However, entrepreneur Yuchen Jin noted that it’s the “slowest and most overthinking model.” Behind the news: OpenAI rolled out o3-pro with a lower price, $20/$80 per 1 million input/output tokens, than o1-pro (which was priced at $150/$600 per 1 million input/output tokens but was deprecated in favor of the new model). Simultaneously it cut the price of o3 by 80 percent to $2/$8 per 1 million input/output tokens. These moves continue the plummeting price of inference over the past year. DeepSeek-R1 offers performance that approaches that of top models for $0.55/$2.19 per 1 million input/output tokens. Why it matters: OpenAI is pushing the limits of current approaches to reasoning, and the results are promising if incremental. o3-pro’s extensive reasoning may appeal to developers who are working on the multi-step scientific problems. For many uses, though, the high price and slow speed may be a dealbreaker. We’re thinking: Letting developers choose between o3 and o3-pro lets them calibrate their computational budget to the difficulty of the task at hand. What if we want to do the same with a trained, open-weights, large language model? Forcing an LLM to generate “Wait” in its output causes it to keep thinking, and can improve its output significantly.",
      "sourceUrl": [
        {
          "text": "nbsp;\nOpenAI launched o3-pro, a more capable version of its most advanced reasoning vision-language model.\nnbsp;\nWhat’s new: o3-pro is designed",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6v3qgyTW7Y8-PT6lZ3mhW2jw_Pt2ym9qyN1TY5qdSrPklW94ywrn7RZmRzW1WqmNC4STPFSW8yYL8d68cvL8W7GtWKF3_zMJDVW8nQX1mDfwZN3jbjy6GLC2cW7wPpkh5XbKW5W7WSScQ2rg0R2Vf7xsW7Jpdw9W2D2g677KFP9xVTNFtj7dJ5MDW7SQQ4_5q9yjlW5nk1-D6MBSkfW4VK2mL6vVmMqW3rV_TN17VW5VW7ZZgM177c8cqW90v7WZ1D9w_4W73tnX63DWcV7W4cGRrb3tT9vrW3jtTv-2qDW6GW1_vjsK5KyqhxW8KcR_Y2Rd6-FW2clpF31vmBvXN8qkD4snB_5jf6-r6PH04"
        },
        {
          "text": "nbsp;to respond to difficult challenges involving science, mathematics, and coding. But its reasoning firepower dramatically slows response times.\n\nInput/output: Text and images in (up to 200,000 tokens), text out (up to 100,000 tokens, 20.7nbsp;tokens per second, 129.2nbsp;seconds to first token",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3mlW1g5c9w8SrxK2W24xt9z63d_DMW4dPfRz6FrR4dW3t9yLW2dn_hZW7VX12j6B5rrsW2XpBCQ5wwVJDW8yTfkc3gtlCJW5mZ9v-5NHSB0W7XTjfn8_w2qTW3v6C8Q3VqkCCW7g1Hsq2cRb8fVPYKCy5WD50cW6VVyzX4_vWDmW1HkCv26jnPTqW35fmGZ3g_1SzW8g0nk_1Nv0zHW79FcH24LTV7zN52Q93fVywNTN6tw8JbKPfzyW3LPqJf4plF8qW3Wp-F71nCbrCW7kH_9Y8qC6NMW2jN_xW8TVP_sW2jgLBs2h45JVf2Yb7cF04"
        },
        {
          "text": ")\nKnowledge cutoff: June 1, 2024\nFeatures: Function calling including web search, structured output\nAvailability/price: Available to ChatGPT Pro and Team users via OpenAI API, soon to Enterprise and Edu users, for $20/$80 per 1 million tokens of input/output\nUndisclosed: Details about architecture, training data, and training methods\n\nPerformance: o3-pro outperformed OpenAI’s own o3 (set to medium effort) and o1-pro in tests performed by OpenAI.\n\nSolving AIME 2024’s advanced high-school math competition problems on the first try, o3-pro (93 percent) bested o3 (90 percent) and o1-pro (86 percent).\nAnswering GPQA Diamond’s graduate-level science questions on the first try, o3-pro (85 percent) outperformed o3 (81 percent) and o1-pro (79 percent).\nCompleting Codeforces competition-coding problems in one pass, o3-pro (2748 CodeElo",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5C3hCZPW69sMD-6lZ3nMW1Qg_1R56LJg_W8VJHQv6yQTwdW1thG-W1bzzjRW7PcCG91T4JtbW22cy7G4K3Jl0Vn0RmY690HM1W2yxZYW56gjRdN8bytc9BKn3ZW3DmpQH6dlFmBN8jlYMJs0L4CN91dBC1ymzC8W9348KP6_j_LKW3PBcJq97NZcNVNrqc52wxjq1N3Lw3nbm2kZlW5HFdds1q5BwCW5bT7dn70Gb1-W1W889Z1z7ll_W4DCGpj2dsbYSW3g-xK43fDwyWf1lpGCx04"
        },
        {
          "text": ") surpassed o3 (2517 CodeElo) and o1-pro (1707 CodeElo).\nIn qualitative tests, human reviewers consistently preferred o3-pro over o3 for queries related to scientific analysis (64.9 percent), personal writing (66.7 percent), computer programming (62.7 percent), and data analysis (64.3 percent).\n\nWhat they’re saying: Reviews of o3-pro so far generally are positive, but the model has been criticized for thenbsp;time it takes to respond. Box CEO Aaron Levie commented",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3mbW66RlZW3QzGx_W1VMjnm43JPmFW6-ZvdZ3Bg4xrW4Yp4_M8k8DFjN5cyjT8q83c9W4MjFk965vgWwW7KtV783qv1cdW50HwP11cpnFbN183_WqzfB8qVtM58m3jTKqXW7s1Cyy6zpDzsW7kLJby6TC1GgW6-BqrP3SjwB9N6dpSH4BwZGVN8gk8fsq-3LhW61dzkl7xJNv4W2fbF7T266KZwW5Lp0jg1JS-4MW4DW3mK2vVxS_N34XgCbbcjShW6D_NR91-HKZCW6nBWgd2FByPRVNDKsC1ZPKNDN7prhFjl6Y2gdqNjXY04"
        },
        {
          "text": "that o3-pro is “crazy good at math and logic.” However, entrepreneur Yuchen Jinnbsp;noted",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3pVN64rvrP6Jc1NW1w1_WV8y54sdW6zZ29H4ykP7CW4sPhK48kvSd8W6XtZz41FYMGXW12wZ9b915S6mW59cJZm3MvmcsW13BkBs1zn5R-W4r63s_4wht2cW1RvzRz6k3dvDW2s7g198VjgPhW7l5-nt6VmjwwW7rS0Wn3cHKWCW7lWxLV6p4jFtW65kW_b7K6cclW3Z30Z64XN_6cW1MWsPp3488x7W6KBG3L4g1w6WW6tPPpv89sHzQW4NSm9y94C-VJW1fV-2W6m2_cPVr06S1936Kq0W4Qtncn7qW4h9W8tRwbs1ywdfRf6TH_Zb04"
        },
        {
          "text": "that it’s the “slowest and most overthinking model.”\nnbsp;\nBehind the news: OpenAI rolled out o3-pro with a lower price, $20/$80 per 1 million input/output tokens, than o1-pro (which was priced at $150/$600 per 1 million input/output tokens but was deprecated in favor of the new model). Simultaneously it cut the price of o3 by 80 percent to $2/$8 per 1 million input/output tokens. These moves continue the plummeting price of inference",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR743qgyTW95jsWP6lZ3nZW6wlD9L9hstWDN6l1mYhrvFJMW16HZwg2dHFVYW2xZ2Zx1k5PCxW2gkn8m6rxZQQW7F6QS17WD7hmN1J2SNBd6_tVV6b46n3v2mTgVTs_Pm1VDSqpW7bD6dR8p73hxVQgBpC5FB_RvN1-563kLqxdHW1n4pfV8YCPjrW7Fq_g81WtfXVW4b7ktN350Ps_W6HmQnZ5Rq6x-W52t_4_2v93ZLVfBW8s6yh7J4W58j9pf8HS-f5W4h6jVQ8X1c76W2ClKd84654sWW2lS3gm7-K0DRW3xPQZp7Ztw28W2gXqf61MdGpSW7lWYSQ7VfBL-V8JTzD4_hHDPW21_2px7QsM-_W86x7gk6mr91dW3H5Qdt3WTKpQN4dPZvmWQC2Sf1gfDBv04"
        },
        {
          "text": "nbsp;over the past year. DeepSeek-R1",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5C5nR32W69t95C6lZ3lqW2W8-XR2274MqN1XpZX7ZDBTqW5M4nwd5xLNKnW85DS0m3hNbyPW67xlrx3ykzn3N3BdqS_-XF4zN3mQCj8j_c6YN7GtC7WVRmfTW93lb-X7c_hHnW8FDzqd3cmbRTW1Blcgg6fl8TbN1PXLb-r8hrkW3wywWk27K1nfW1vMJtj1dZmqWW3fYgPR6ZPvtNW5ssnkZ6SpKh8W2PmkwL171YnVW3mqZn4233NlSW25QPl78NRgNMW4fDyqh4YDMScW8H3lz03DyNTBW2rWYDK8_Qmt7W2llHJ93ZGX3YW6zTjGM2j-l5TW59_vzG8Dw_gSW7d3FDf3mGcStW2z2sFs7_ksytW2zc4C18B-W6cVXJZlp2dvFqCW1v3sfD4qQ79PW84TQsc7VHj8_W1WMmJH98WTTNW42LMcd3xcMKTVBWWgZ60LkfgW50fcls8gldXkW7BG5hc6MyHkQf3zCxP-04"
        },
        {
          "text": "nbsp;offers performance that approaches that of top models for $0.55/$2.19 per 1 million input/output tokens.\nnbsp;\nWhy it matters: OpenAI is pushing the limits of current approaches to reasoning, and the results are promising if incremental. o3-pro’s extensive reasoning may appeal to developers who are working on the multi-step scientific problems. For many uses, though, the high price and slow speed may be a dealbreaker.\nnbsp;\nWe’re thinking: Letting developers choose between o3 and o3-pro lets them calibrate their computational budget to the difficulty of thenbsp;task at hand. What if we want to do the same with a trained,nbsp;open-weights, large language model? Forcing an LLM to generate “Wait",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR743qgyTW95jsWP6lZ3nJW5wTmZb54ssbNW7C2r6T9dw9RDW3sZK4l2KTzSDW7Tv0dl8dpMmRW5Ls__V69Jr91V-6FD04Mw5s6VfswyV3vlpkYW8c7r6c7L4ncFN6KPkM_3YdhgW3NgTLN2ZTT4FW7HmdMM72gdLjW73w2S59dCGFkW6Hbb_J81hPKhW3T9X9C7ZGqh7W1L31Rv6myDmVW880-yQ5PY05WN8QkZXtD8GJzW6YVkVk6vBYRhW4vv-Zp5FpnS5TRLLD9bMKJCW2Xrm_94lj3hVN5ZSJSt4nrBWW1hvf7z2PqTYMW6k0Kq14nz82XW2rwCb58pr4h6W4FDj7K5qTty9W2txcvg51_zR8W3_gFRN3lNt05W6Mvzhw1D3LD5W7SG3YX2ydSDcf89Tj8W04"
        }
      ]
    },
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "LLM Rights Historical Wrongs",
      "content_String": "In Northern California, old property deeds may still include racial clauses: language, made illegal decades ago, that was designed to ban people of color from owning or living in certain homes. The state of California now requires counties to find and remove them, but manually combing through millions of documents would take years. Researchers used AI to find them automatically. What’s new: Faiz Surani, Mirac Suzgun, and colleagues at Stanford University and Princeton University fine-tuned a large language model to find racial clauses in deeds for property in the California county of Santa Clara. Key insight: Manual and keyword searches may fail to catch racial clauses if they’re obscured by subtle wording or errors in optical character recognition (OCR). But a fine-tuned large language model can understand context, identify relevant phrases, and avoid potential false alarms like the surnames Black or White. Lawyers can confirm the model’s findings. How it works: The authors used an OCR system to extract text from 5.2 million pages of Santa Clara property deeds filed between 1850 and 1980. They drew examples from that corpus to form training and validation datasets and then processed the rest to find deeds that contained racial clauses. To curate examples for training and validation, the authors started by sampling 20,000 pages at random. Since deeds have significant variation in format and quality, they added 10,000 deeds from other U.S. counties. They filtered the combined examples using keywords that may indicate racial clauses, such as “Negro,” “Mongolian,” or “No person of,” yielding 3,801 pages. They manually labeled the spans that included such language, which appeared on roughly 80 percent of those pages. They fine-tuned Mistral-7B via LoRA on the labeled examples to learn to detect and reproduce discriminatory text. Results: The authors fed the remaining roughly 5.2 million unlabeled pages to the fine-tuned model. When the model identified a deed that contained a racial clause, county staff confirmed the finding and redacted the clause. The authors found 24,500 Santa Clara lots covered by racial clauses — about one in four homes in the county in 1950. It also revealed that 10 developers, out of what the authors estimate were hundreds, were responsible for one-third of the racial clauses, demonstrating that only a small number of actors shaped decades of segregation. The fine-tuned model reviewed all pages in 6 days, which would cost an estimated $258 based on current prices for cloud access to GPUs. In contrast, few-shot prompting GPT-3.5 Turbo would have been faster (3.6 days) but less accurate and over 50 times more expensive ($13,634). Working manually, a single county staff member would have needed nearly 10 years and $1.4 million. Why it matters: Large language models can interpret historical documents to reveal the nature and scope of actions in the past that otherwise would remain obscure — in this case, housing discrimination. By flagging discriminatory language, this work enables historians to identify areas affected by racial clauses and trace their broader social and economic effects. The team open-sourced the model, streamlining the process for other United States counties. We’re thinking: While AI is making history, it’s also illuminating it!",
      "sourceUrl": [
        {
          "text": "nbsp;\nIn Northern California, old property deeds may still include racial clauses: language, made illegal decades ago, that was designed to ban people of color from owning or living in certain homes. The state of California now requires counties to find and remove them, but manually combing through millions of documents would take years. Researchers used AI to find them automatically.\nnbsp;\nWhat’s new: Faiz Surani, Mirac Suzgun, and colleagues at Stanford University and Princeton University fine-tuned a large language modelnbsp;to find racial clauses",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3lrN3fW4HjDqNyGN2-YnkF9nFd1W1vxls_81z4C0W8LdPKy6L1Q3bW5yGBMC2VG1VRW6Bc7_d4XV-phVG93h83Q3lK3W1vLBcX88Y-5xN3tdDD43H5H5W2w03j54l7zXdW6cLvRg1NWRv1W7lFZN98N53x6W50dhhY3GYwJDW5mp8yW1hQRRBW4c5c4b7NpcZZW3hv44H1ZJTmSW4FN5Vq4HmsDWVhBW_L8w8HCJW6f4vk32sRvrZN6JyjmKmtqHNW1mgS-k7dYjHpW6HLtdY17kC74f98FFN204"
        },
        {
          "text": "in deeds for property in the California county of Santa Clara.\nnbsp;\nKey insight: Manual and keyword searches may fail to catch racial clauses if they’re obscured by subtle wording or errors in optical character recognition (OCR). But a fine-tuned large language modelnbsp;can understand context, identify relevant phrases, and avoid potential false alarms like the surnames Black or White. Lawyers can confirm the model’snbsp;findings.\nnbsp;\nHow it works: The authors used an OCR",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3ntW4BMyBj6hchgTW6wxh082SHbtKW86797t3Nw_DWW7QzgJs9hxhxkW2-XFGJ5byrQkVXj9-d2FpwcrW6NBpZv3tqRcsW9b5rRQ7VJwPtW7BQttd5lJB9lW4RtCc_1HDcBgW2zWf4z2sWW5pW4RTmkX2fPgCgN4Sh3_PZW26mN3Y5S8sYywcRW3vCC_j4tpzzRW75gsd56VNnd2W5Yl9pK5CCmQ0W4cmJXZ83tkPfW6FZh9d1MsmfNVBj-ZH455H-GW7DJy7j89_lt3W5Hy9D76lSjDyf8rrJC-04"
        },
        {
          "text": "system to extract text from 5.2 million pages of Santa Claranbsp;property deeds filed between 1850 and 1980. They drew examples from that corpus to form training and validation datasets and then processed the rest to find deeds that contained racial clauses.\n\nTo curate examples for training and validation, the authors started by sampling 20,000 pages at random. Since deeds have significant variation in format and quality, they added 10,000 deeds from other U.S. counties",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR6b3qgyTW7lCdLW6lZ3p6W4my14r84h62rW4s0pJN2pn2vVN8wkPSG5L7RjW1d4h7b48djVrW8ZdryM5cDmlMV9n7Gk4y_n44W90fFWm1wrGX9W21k_v15hkCskW3qtcxK8z2TF-W3vTQWC23hkK-W8QNRyF7_T9gYW68Xw3R87CPYZN47qTsSgNwXRW74dHTK77Nd8fN7HK91yyy2JqW1M6wGd71gSc-W1KT34z7VQzfbW5yVpy47WbzWdW2ypmyD8WqmqWW5j-08Q1lBhP4W503FhW3-_j_vW5BZ7Yn2mkskpW6Yyq_H37bGdtW4JsBtb904WYVf4672BK04"
        },
        {
          "text": ".\nThey filtered the combined examples using keywords that may indicate racial clauses, such as “Negro,” “Mongolian,” or “No person of,” yielding 3,801 pages.\nThey manually labeled the spans that included such language, which appeared on roughly 80 percent of those pages.\nThey fine-tuned Mistral-7B",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3ksW2bH6Gm4MSR00W8WkMjS5HfLB5W9fqF625rrLTKW2CStc942m8Z-F3t3PYM8hdvW2NZb3S63zg5yV11gfk75n5fXW4D2-D194WgLrN1ccgy-32pgjW6Gjx-Z9kGbQkW2hpFJ65vbDKpW237-848X0SJpW3hnb9f6h7VSHW369mQR2qvBtXW32Msm-5hrBMhW2NSS5K44JlfPW63pJd124kBNlVxTFbC2L4kpBW42CLsw4-hhxHN25NVGBdB3jwW9gRf9B5JGzQVW7GXHQ88Y9xT0f3j3FWW04"
        },
        {
          "text": "nbsp;via LoRA",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3mWN3MF7HHvQPZmW5l6P2-4zx7DGW8DQwln9cH4qCW6tRZ5l3Byb0DVTq4VD4gkp6KW9cy9sH40BSX1W3Pr8T02wZPyCW3JBfh41hfc4tW49PXZd1rZXsFW7fKMFr6gkgC5W7q3M2p4FDDTBV_Jtyt30_KYhW6-NVct5V3V31W1fD_p1269GtYN9lR6W2pdk1HW7RYhYW1_21JmW87ZQkX29hSgRW8qxClM3513h1Vk7hvQ19vbVgW73Z9MX7ND9vhN2mFMqhx_GZMN6-fyCyTDgScf1gMV7P04"
        },
        {
          "text": "on the labeled examples to learn to detect and reproduce discriminatory text.\n\nResults: The authors fed the remaining roughly 5.2 million unlabeled pages to the fine-tuned model. When the model identified a deed that contained a racial clause, county staff confirmed the finding and redacted the clause.\n\nThe authors found 24,500 Santa Clara lots covered by racial clauses — about one in four homes in the county in 1950.\nIt also revealed that 10 developers, out of what the authors estimate were hundreds, were responsible for one-third of the racial clauses, demonstrating that only a small number of actors shaped decades of segregation.\nThe fine-tuned model reviewed all pages in 6 days, which would cost an estimated $258 based on current prices for cloud access to GPUs. In contrast, few-shot prompting GPT-3.5 Turbo would have been faster (3.6 days) but less accurate and over 50 times more expensive ($13,634). Working manually, a single county staff member would have needed nearly 10 years and $1.4 million.\n\nWhy it matters: Large language models can interpret historical documents to reveal the nature and scope of actions in the past that otherwise would remain obscure — in this case, housing discrimination. By flagging discriminatory language, this work enables historians to identify areas affected by racial clauses and trace their broader social and economic effects. The team open-sourced",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3lbW4NKp6z3WKQ4QW2ybH_W5Hl2vyVT31P12Z6ZBCN8bzd66GWrRBW2hpYFC3ndBKbW2bl3CP65h9JFW4qxSnq3myKTyW1f5J3f3lX7q7N6pF88K7Q_G6W1Vh4GX3tF0HYW4BbCZH187Ms7W6ng8GP73KbvxW6_01sJ4XYkrTW8tVpfT26-gJ_W20XTkC5qDCkPW7Ky8R_8z_kvXW9jC77P6WW2gWN2txr0lZnmjbV8LhHp5QXYsNW4YpKv93v-ZBGN7gFncXZqfzZN1l8g4jx2WTxf2SX3N604"
        }
      ]
    },
    {
      "segment": "",
      "contentType": "",
      "contentId": "",
      "contentTitle": "Work With Andrew Ng",
      "content_String": "Join the teams that are bringing AI to the world! Check out job openings at DeepLearning.AI, AI Fund, and Landing AI.",
      "sourceUrl": [
        {
          "text": "nbsp;\nJoin the teams that are bringing AI to the world! Check out job openings atnbsp;DeepLearning.AI",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3nCW8R3PLw7qTr6yW2545T91hdnhcF2nzdJJ2qjyW4gy5h335sv4sW7tLkmJ1x7QT-W5K_Lxm3r940ZW6M6kH_7fJn9bVh1pDt3V05H3N39lQ51lFSn1W4mVX268BYXwKW1kGhT55MxTZXW1Q-Cp92sFPBwW496hZm2qyqkwW4xfqLP2-8_fqW8V5LQZ2K-hflW272MHf7S7C4zVBp3MC2HddpWW5N8WDd1szF5DW3ZSKjT27FX3vW8xD8VV4wWj4mW7RHTV685Mt43W3j6jxX3SMZgrf4rLdYR04"
        },
        {
          "text": ",nbsp;AI Fund",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3pWW4v9CK-7zXnrHW4FDbGd5Blfh6W1cFYNh6j_Wh3V5B8lZ6GcclxVWKHKz3jXPHRVZ1TFD2LdBy4W37gtwR8xj7YtW8dFj-y8PGX4LVTsg-74TY1N9W34LZLS5PVhqdW2Gk--g96hf4nV3g4JG7FDzGSW5c4_Yd3dZMxKW6k362Y7ykgT5W61nkYn94rmPVW93rdM-4R3jLwVR-Bsy2DHc2WW2RPL3_1LvB6kN4KPBST8hhg2W7zJGXd4-cnPXW67MB2f1-S6tgW8_140c4cTvw0f4S0mQY04"
        },
        {
          "text": ", andnbsp;Landing AI",
          "url": "https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVT4Tz4cXch9W4jbMBT6SQDBSW2bnPc75x_vLzN8BHR5W3qgyTW6N1vHY6lZ3kPW5c9CVh5MRCQYW7fHCxm3pfllRW8jdQh_6kP_vQV42Wl15sBrthW7TQVgB7XpjsgW25-bDQ3fcsvZW1Zx4wL56jq9nW2zY19Q85bqXlW688X_04Lb945W8cr0wl5tD_gnW97YDJW3g-tnNN4rzXkhb36H7W6BWhRd4WwRRQW3MqSJS2Yl84CN5lk5z7z24b_V5XBYB8kMbp9W8v-dhQ7_-K-QW80QZKw8hxPjCW9bmF8S2WrtJvW3YVCq19cs3XMW26y_2v4wL_BfN2Xkx256FmkQf7CWK0804"
        }
      ]
    }
  ],
  "extractedAt": "2025-07-24T17:42:21.469Z"
}